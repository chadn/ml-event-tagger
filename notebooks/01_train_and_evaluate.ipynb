{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Event Tagger - Training & Evaluation\n",
        "\n",
        "This notebook demonstrates the complete training pipeline and provides detailed evaluation metrics for the multi-label event classification model.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **Goal:** Train a TensorFlow/Keras model to predict event tags\n",
        "- **Architecture:** Embedding → Pooling → Dense layers → Sigmoid output\n",
        "- **Data:** 100 labeled events, 21 tags\n",
        "- **Split:** 70% train, 15% validation, 15% test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
        "\n",
        "# Import our modules\n",
        "from ml_event_tagger.config import TAGS\n",
        "from ml_event_tagger.model import create_model\n",
        "from ml_event_tagger.train import (\n",
        "    load_preprocessed_data,\n",
        "    create_tokenizer,\n",
        "    tokenize_texts\n",
        ")\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(f\"✅ Imports loaded\")\n",
        "print(f\"📊 Number of tags: {len(TAGS)}\")\n",
        "print(f\"🏷️  Tags: {', '.join(TAGS)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed data\n",
        "train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, tag_names = load_preprocessed_data()\n",
        "\n",
        "print(f\"✅ Data loaded:\")\n",
        "print(f\"   Train: {len(train_texts)} samples\")\n",
        "print(f\"   Val:   {len(val_texts)} samples\")\n",
        "print(f\"   Test:  {len(test_texts)} samples\")\n",
        "print(f\"   Tags:  {len(tag_names)} tags\")\n",
        "print()\n",
        "\n",
        "# Show sample\n",
        "print(\"Sample preprocessed text:\")\n",
        "print(f\"  '{train_texts[0][:100]}...'\")\n",
        "print()\n",
        "print(\"Sample labels:\")\n",
        "sample_tags = [tag for i, tag in enumerate(TAGS) if train_labels[0][i] == 1]\n",
        "print(f\"  Tags: {sample_tags}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenize Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and fit tokenizer\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "tokenizer = create_tokenizer(train_texts, vocab_size=MAX_VOCAB_SIZE)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"✅ Tokenizer created\")\n",
        "print(f\"   Vocabulary size: {vocab_size:,} words\")\n",
        "print(f\"   Max sequence length: {MAX_LENGTH}\")\n",
        "print()\n",
        "\n",
        "# Tokenize\n",
        "X_train = tokenize_texts(tokenizer, train_texts, max_length=MAX_LENGTH)\n",
        "X_val = tokenize_texts(tokenizer, val_texts, max_length=MAX_LENGTH)\n",
        "X_test = tokenize_texts(tokenizer, test_texts, max_length=MAX_LENGTH)\n",
        "\n",
        "print(f\"✅ Texts tokenized:\")\n",
        "print(f\"   X_train: {X_train.shape}\")\n",
        "print(f\"   X_val:   {X_val.shape}\")\n",
        "print(f\"   X_test:  {X_test.shape}\")\n",
        "print()\n",
        "\n",
        "# Show most common words\n",
        "word_freq = sorted(tokenizer.word_index.items(), key=lambda x: x[1])[:20]\n",
        "print(\"Top 20 most common words:\")\n",
        "print(\"  \", \", \".join([w for w, _ in word_freq]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Trained Model\n",
        "\n",
        "We'll load the model that was trained using `train.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "model = keras.models.load_model(\"models/event_tagger_model.h5\")\n",
        "\n",
        "print(\"✅ Model loaded successfully\")\n",
        "print()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "test_results = model.evaluate(X_test, test_labels, verbose=0)\n",
        "\n",
        "print(\"📊 Test Set Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Loss:            {test_results[0]:.4f}\")\n",
        "print(f\"  Binary Accuracy: {test_results[1]:.4f} ({test_results[1]*100:.1f}%)\")\n",
        "print(f\"  Precision:       {test_results[2]:.4f} ({test_results[2]*100:.1f}%)\")\n",
        "print(f\"  Recall:          {test_results[3]:.4f} ({test_results[3]*100:.1f}%)\")\n",
        "\n",
        "# Calculate F1 score\n",
        "precision = test_results[2]\n",
        "recall = test_results[3]\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "print(f\"  F1 Score:        {f1:.4f} ({f1*100:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Check success criteria\n",
        "print(\"✅ Success Criteria:\")\n",
        "if precision >= 0.60:\n",
        "    print(f\"  ✅ Precision ≥ 60%: {precision*100:.1f}% (PASSED)\")\n",
        "else:\n",
        "    print(f\"  ❌ Precision ≥ 60%: {precision*100:.1f}% (FAILED)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Make Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "y_pred_proba = model.predict(X_test, verbose=0)\n",
        "\n",
        "# Convert to binary predictions (threshold = 0.5)\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "print(f\"✅ Predictions generated\")\n",
        "print(f\"   Shape: {y_pred.shape}\")\n",
        "print(f\"   Predicted tags per event: {y_pred.sum(axis=1).mean():.1f} (average)\")\n",
        "print(f\"   Actual tags per event:    {test_labels.sum(axis=1).mean():.1f} (average)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training History Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training history\n",
        "with open('models/training_history.json') as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Training History', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Loss\n",
        "ax = axes[0, 0]\n",
        "ax.plot(epochs, history['loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "ax.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Binary Crossentropy Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "ax = axes[0, 1]\n",
        "ax.plot(epochs, history['binary_accuracy'], 'b-', label='Train Accuracy', linewidth=2)\n",
        "ax.plot(epochs, history['val_binary_accuracy'], 'r-', label='Val Accuracy', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Binary Accuracy')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Precision\n",
        "ax = axes[1, 0]\n",
        "ax.plot(epochs, history['precision'], 'b-', label='Train Precision', linewidth=2)\n",
        "ax.plot(epochs, history['val_precision'], 'r-', label='Val Precision', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Recall\n",
        "ax = axes[1, 1]\n",
        "ax.plot(epochs, history['recall'], 'b-', label='Train Recall', linewidth=2)\n",
        "ax.plot(epochs, history['val_recall'], 'r-', label='Val Recall', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Recall')\n",
        "ax.set_title('Recall')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"✅ Training completed after {len(epochs)} epochs\")\n",
        "print(f\"   Final validation loss: {history['val_loss'][-1]:.4f}\")\n",
        "print(f\"   Final validation precision: {history['val_precision'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Per-Tag Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate per-tag metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "per_tag_metrics = []\n",
        "for i, tag in enumerate(TAGS):\n",
        "    y_true_tag = test_labels[:, i]\n",
        "    y_pred_tag = y_pred[:, i]\n",
        "\n",
        "    # Skip tags with no positive samples\n",
        "    if y_true_tag.sum() == 0:\n",
        "        per_tag_metrics.append({\n",
        "            'tag': tag,\n",
        "            'count': 0,\n",
        "            'precision': 0,\n",
        "            'recall': 0,\n",
        "            'f1': 0\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    prec = precision_score(y_true_tag, y_pred_tag, zero_division=0)\n",
        "    rec = recall_score(y_true_tag, y_pred_tag, zero_division=0)\n",
        "    f1_tag = f1_score(y_true_tag, y_pred_tag, zero_division=0)\n",
        "\n",
        "    per_tag_metrics.append({\n",
        "        'tag': tag,\n",
        "        'count': int(y_true_tag.sum()),\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1_tag\n",
        "    })\n",
        "\n",
        "# Display table\n",
        "print(\"Per-Tag Performance on Test Set:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Tag':<12} {'Count':<7} {'Precision':<12} {'Recall':<12} {'F1 Score':<12}\")\n",
        "print(\"-\" * 70)\n",
        "for m in per_tag_metrics:\n",
        "    if m['count'] > 0:\n",
        "        print(f\"{m['tag']:<12} {m['count']:<7} {m['precision']:<12.2f} {m['recall']:<12.2f} {m['f1']:<12.2f}\")\n",
        "    else:\n",
        "        print(f\"{m['tag']:<12} {m['count']:<7} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Per-Tag Precision/Recall Bar Chart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter tags with samples\n",
        "tags_with_samples = [m for m in per_tag_metrics if m['count'] > 0]\n",
        "\n",
        "if len(tags_with_samples) > 0:\n",
        "    tags = [m['tag'] for m in tags_with_samples]\n",
        "    precisions = [m['precision'] for m in tags_with_samples]\n",
        "    recalls = [m['recall'] for m in tags_with_samples]\n",
        "\n",
        "    x = np.arange(len(tags))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    bars1 = ax.bar(x - width/2, precisions, width, label='Precision', color='steelblue', alpha=0.8)\n",
        "    bars2 = ax.bar(x + width/2, recalls, width, label='Recall', color='coral', alpha=0.8)\n",
        "\n",
        "    ax.set_xlabel('Tag', fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontweight='bold')\n",
        "    ax.set_title('Per-Tag Precision and Recall on Test Set', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(tags, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    ax.set_ylim(0, 1.1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            if height > 0:\n",
        "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                       f'{height:.2f}',\n",
        "                       ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️  No tags with test samples to plot\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Tag Frequency Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count tag frequencies in test set\n",
        "tag_counts = test_labels.sum(axis=0)\n",
        "tag_freq = [(TAGS[i], int(tag_counts[i])) for i in range(len(TAGS)) if tag_counts[i] > 0]\n",
        "tag_freq = sorted(tag_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "if len(tag_freq) > 0:\n",
        "    tags, counts = zip(*tag_freq)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    bars = ax.bar(range(len(tags)), counts, color='teal', alpha=0.7)\n",
        "    ax.set_xlabel('Tag', fontweight='bold')\n",
        "    ax.set_ylabel('Frequency', fontweight='bold')\n",
        "    ax.set_title('Tag Frequency Distribution in Test Set', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(range(len(tags)))\n",
        "    ax.set_xticklabels(tags, rotation=45, ha='right')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add count labels\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{int(height)}',\n",
        "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTag frequencies in test set:\")\n",
        "    for tag, count in tag_freq:\n",
        "        print(f\"  {tag:<12} {count:>3}\")\n",
        "else:\n",
        "    print(\"⚠️  No tags found in test set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Sample Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show a few sample predictions\n",
        "print(\"Sample Predictions:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i in range(min(5, len(test_texts))):\n",
        "    print(f\"\\nEvent {i+1}:\")\n",
        "    print(f\"  Text: '{test_texts[i][:100]}...'\")\n",
        "\n",
        "    # Actual tags\n",
        "    actual_tags = [TAGS[j] for j in range(len(TAGS)) if test_labels[i][j] == 1]\n",
        "    print(f\"  Actual tags:    {actual_tags}\")\n",
        "\n",
        "    # Predicted tags\n",
        "    predicted_tags = [TAGS[j] for j in range(len(TAGS)) if y_pred[i][j] == 1]\n",
        "    print(f\"  Predicted tags: {predicted_tags}\")\n",
        "\n",
        "    # Top 5 predictions with confidence\n",
        "    top5_indices = np.argsort(y_pred_proba[i])[-5:][::-1]\n",
        "    top5_tags = [(TAGS[j], y_pred_proba[i][j]) for j in top5_indices]\n",
        "    print(f\"  Top 5 (with confidence):\")\n",
        "    for tag, conf in top5_tags:\n",
        "        print(f\"    {tag:<12} {conf:.3f}\")\n",
        "\n",
        "    # Check correctness\n",
        "    correct = set(actual_tags) == set(predicted_tags)\n",
        "    if correct:\n",
        "        print(\"  ✅ Perfect match!\")\n",
        "    else:\n",
        "        missed = set(actual_tags) - set(predicted_tags)\n",
        "        extra = set(predicted_tags) - set(actual_tags)\n",
        "        if missed:\n",
        "            print(f\"  ⚠️  Missed: {missed}\")\n",
        "        if extra:\n",
        "            print(f\"  ⚠️  Extra: {extra}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary & Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING & EVALUATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"📊 Dataset:\")\n",
        "print(f\"  Total events: {len(train_texts) + len(val_texts) + len(test_texts)}\")\n",
        "print(f\"  Train: {len(train_texts)} | Val: {len(val_texts)} | Test: {len(test_texts)}\")\n",
        "print(f\"  Number of tags: {len(TAGS)}\")\n",
        "print()\n",
        "print(\"🧠 Model:\")\n",
        "print(f\"  Architecture: Embedding → Pooling → Dense layers → Sigmoid\")\n",
        "print(f\"  Vocabulary size: {vocab_size:,} words\")\n",
        "print(f\"  Max sequence length: {MAX_LENGTH}\")\n",
        "print(f\"  Parameters: ~107K\")\n",
        "print()\n",
        "print(\"📈 Performance:\")\n",
        "print(f\"  Test Precision: {test_results[2]:.1%}\")\n",
        "print(f\"  Test Recall:    {test_results[3]:.1%}\")\n",
        "print(f\"  Test F1 Score:  {f1:.1%}\")\n",
        "print(f\"  Test Accuracy:  {test_results[1]:.1%}\")\n",
        "print()\n",
        "print(\"✅ Success Criteria:\")\n",
        "if test_results[2] >= 0.60:\n",
        "    print(f\"  ✅ Precision ≥ 60%: {test_results[2]:.1%} (PASSED)\")\n",
        "else:\n",
        "    print(f\"  ❌ Precision ≥ 60%: {test_results[2]:.1%} (FAILED)\")\n",
        "print()\n",
        "print(\"💡 Insights:\")\n",
        "print(f\"  • High precision ({test_results[2]:.1%}) = Model is confident when it predicts\")\n",
        "print(f\"  • Moderate recall ({test_results[3]:.1%}) = Model is conservative\")\n",
        "print(f\"  • Good results for only {len(train_texts)} training samples!\")\n",
        "print(f\"  • Performance will improve with more data (see ROADMAP v0.2)\")\n",
        "print()\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
